{
  "services": [
    {
      "id": 1,
      "name": "Relais",
      "description": "Record Linkage at Istat",
      "organization": "Istat"
    },
    {
      "id": 2,
      "name": "Validate",
      "description": "Data validation in R",
      "organization": "Istat"
    },
    {
      "id": 3,
      "name": "ARC loader",
      "description": "Arc file loader",
      "organization": "Istat"
    }
  ],
  "processes": [
    {
      "id": 1,
      "name": "Prob",
      "description": "Probabilistic record linkage",
      "label": "PRLblock",
      "organization": "Istat",
      "graph": {
        "nodes": [
          {
            "id": 1604917567907,
            "x": 50,
            "y": 50,
            "type": "operation",
            "idnode": 10,
            "name": "MLEST",
            "description": "This function performs the maximum likelihood estimates of the parameters of a contamination model by ECM algorithm and it provides the expected values of the \"true\" data for all units that were used for the estimation",
            "width": 120,
            "height": 60
          },
          {
            "id": 1604917569333,
            "x": 260,
            "y": 110,
            "type": "operation",
            "idnode": 20,
            "name": "PREDY",
            "description": "On the basis of a set of contamination model parameters, and a set of observed data, it calculates the expected values of the corresponding real data. Missing values for the variables response as well as are allowed, but not for covariates",
            "width": 120,
            "height": 60
          },
          {
            "id": 1604917594005,
            "x": 430,
            "y": 220,
            "type": "operation",
            "idnode": 71,
            "name": "FELLEGI_SUNTER",
            "description": "The Fellegi and Sunter method is a probabilistic approach to solve record linkage problem based on decision model.  According to the method, given two (or more) sources of data, all pairs coming from the Cartesian product of the two sources has to be classified in three independent and mutually exclusive subsets: the set of matches, the set of non-matches and the set of pairs requiring manual review. In order to classify the pairs, the comparisons on common attributes are used to estimate for each pair the probabilities to belong to both the set of matches and the set of non-matches. The pair classification criteria is based on the ratio between such conditional probabilities. The decision model aims to minimize both the misclassification errors and the probability of classifying a pair as belonging to the subset of pairs requiring manual review",
            "width": 120,
            "height": 60
          },
          {
            "id": 1605261463065,
            "x": 200,
            "y": 220,
            "type": "operation",
            "idnode": 71,
            "name": "FELLEGI_SUNTER",
            "description": "The Fellegi and Sunter method is a probabilistic approach to solve record linkage problem based on decision model.  According to the method, given two (or more) sources of data, all pairs coming from the Cartesian product of the two sources has to be classified in three independent and mutually exclusive subsets: the set of matches, the set of non-matches and the set of pairs requiring manual review. In order to classify the pairs, the comparisons on common attributes are used to estimate for each pair the probabilities to belong to both the set of matches and the set of non-matches. The pair classification criteria is based on the ratio between such conditional probabilities. The decision model aims to minimize both the misclassification errors and the probability of classifying a pair as belonging to the subset of pairs requiring manual review",
            "width": 120,
            "height": 60
          }
        ],
        "connections": [
          {
            "source": {
              "id": 1604917567907,
              "position": "right"
            },
            "destination": {
              "id": 1604917569333,
              "position": "left"
            },
            "id": 1604917574424,
            "type": "pass",
            "name": "Pass"
          },
          {
            "source": {
              "id": 1604917569333,
              "position": "right"
            },
            "destination": {
              "id": 1604917594005,
              "position": "top"
            },
            "id": 1604917597934,
            "type": "pass",
            "name": "Pass"
          },
          {
            "source": {
              "id": 1604917594005,
              "position": "left"
            },
            "destination": {
              "id": 1605261463065,
              "position": "right"
            },
            "id": 1605261471371,
            "type": "pass",
            "name": "Pass"
          }
        ]
      }
    },
    {
      "id": 2,
      "name": "Deterministic record linkage",
      "description": "Deterministic record linkage",
      "label": "DRL",
      "organization": "Istat"
    },
    {
      "id": 3,
      "name": "Probabilistic record linkage with blocking",
      "description": "Probabilistic record linkage with blocking",
      "label": "PRLblock",
      "organization": "Istat"
    },
    {
      "id": 4,
      "name": "Data validation in R",
      "description": "Data validation in R",
      "label": "ValidateR",
      "organization": "Istat"
    }
  ],
  "users": [
    {
      "id": 1,
      "email": "framato@istat.it",
      "name": "Francesco",
      "surname": "Amato",
      "role": "ADMIN"
    },
    {
      "id": 2,
      "email": "iannacone@istat.it",
      "name": "Renzo",
      "surname": "Iannacone",
      "role": "USER"
    },
    {
      "id": 3,
      "email": "macone@istat.it",
      "name": "Stefano",
      "surname": "Macone",
      "role": "ADMIN"
    },
    {
      "id": 4,
      "email": "papizzo@istat.it",
      "name": "Paolo",
      "surname": "Pizzo",
      "role": "USER"
    }
  ],
  "business_functions": [
    {
      "id": 1,
      "name": "Record Linkage",
      "descr": "The purpose of record linkage is to identify the same real world entity that can be differently represented in data sources, even if unique identifiers are not available or are affected by errors.",
      "label": "RL",
      "active": 1
    },
    {
      "id": 2,
      "name": "Data Editing",
      "descr": "Data editing is the process of reviewing the data for consistency, detection of errors and outliers and correction of errors, in order to improve the quality, accuracy and adequacy of the data and make it suitable for the purpose for which it was collected.",
      "label": "EDIT",
      "active": 1
    },
    {
      "id": 3,
      "name": "Data Validation",
      "descr": "Data validation is the process of ensuring data have undergone data cleansing to ensure they have data quality, that is, that they are both correct and useful. It uses routines, often called \"validation rules\", that check for correctness, meaningfulness, and security of data that are input to the system.",
      "label": "VALidATE",
      "active": 1
    }
  ],
  "business_processes": [
    {
      "id": 1,
      "name": "Probabilistic Record Linkage",
      "descr": "Probabilistic Record Linkage",
      "label": "PRL",
      "order_code": 1,
      "parent": null
    },
    {
      "id": 2,
      "name": "Deterministic Record Linkage",
      "descr": "Deterministic Record Linkage",
      "label": "DRL",
      "order_code": 2,
      "parent": null
    },
    {
      "id": 3,
      "name": "R data validation",
      "descr": "R data validation",
      "label": "ValidateR",
      "order_code": 1,
      "parent": null
    },
    {
      "id": 4,
      "name": "Data validation Van der Loo",
      "descr": "Data validation Van der Loo",
      "label": "VanDerLoo",
      "order_code": 1,
      "parent": 3
    },
    {
      "id": 5,
      "name": "Probabilistic Record Linkage MultiStep",
      "descr": "Probabilistic Record Linkage MultiStep",
      "label": "PRL-MS",
      "order_code": 3,
      "parent": null
    },
    {
      "id": 6,
      "name": "Probabilistic Record Linkage MultiStep",
      "descr": "Probabilistic Record Linkage MultiStep",
      "label": "PRL-MS1",
      "order_code": 4,
      "parent": 5
    },
    {
      "id": 70,
      "name": "Contingency Table",
      "descr": "Calculate contingency table",
      "label": "CrossTable",
      "order_code": 1,
      "parent": 1
    },
    {
      "id": 71,
      "name": "Fellegi Sunter",
      "descr": "Fellegi Sunter algorithm",
      "label": "FellegiSunter",
      "order_code": 2,
      "parent": 1
    },
    {
      "id": 72,
      "name": "Matching Table",
      "descr": "Matching records",
      "label": "MatchingTable",
      "order_code": 3,
      "parent": 1
    },
    {
      "id": 80,
      "name": "Selezione errori influenti",
      "descr": "Esegue la stima, predizione e valuta gli errori influenti in due processi successivi",
      "label": "Selezione2P",
      "order_code": 1,
      "parent": null
    },
    {
      "id": 110,
      "name": "Stima e predizione modello",
      "descr": "Escuzione del processo di stima e predizione del modello",
      "label": "Estimates",
      "order_code": 1,
      "parent": 80
    },
    {
      "id": 130,
      "name": "Identificazione errori influenti",
      "descr": "Esecuzione del processo di identificazione errori influenti",
      "label": "Selection",
      "order_code": 2,
      "parent": 80
    }
  ],
  "business_services": [
    {
      "id": 100,
      "name": "Selective editing",
      "descr": "Selective editing is a general approach for the detection of influential errors. It is based on the idea of looking for influential errors with respect to the main results in order to focus the most accurate treatment on the corresponding subset of units to limit the costs of interactive editing, while maintaining the desired level of quality of estimates [GSDEM 2.0]",
      "gsbpm_process_id": 53
    },
    {
      "id": 200,
      "name": "Record linkage",
      "descr": "The purpose of record linkage is to identify the same real world entity that can be differently represented in data sources, even if unique identifiers are not available or are affected by errors.",
      "gsbpm_process_id": 51
    },
    {
      "id": 300,
      "name": "Data validation",
      "descr": "An activity aimed at verifying whether the value of a data item comes from the given (finite or infinite) set of acceptable values [OECD]",
      "gsbpm_process_id": 53
    }
  ],
  "process_steps": [
    {
      "id": 10,
      "name": "MLEST",
      "label": "MLEST",
      "descr": "This function performs the maximum likelihood estimates of the parameters of a contamination model by ECM algorithm and it provides the expected values of the \"true\" data for all units that were used for the estimation",
      "business_service_id": "100"
    },
    {
      "id": 15,
      "name": "MLEST_STRATA",
      "label": "MLEST_STRATA",
      "descr": "MLEST with stratification",
      "business_service_id": 100
    },
    {
      "id": 20,
      "name": "PREDY",
      "label": "PREDY",
      "descr": "On the basis of a set of contamination model parameters, and a set of observed data, it calculates the expected values of the corresponding real data. Missing values for the variables response as well as are allowed, but not for covariates",
      "business_service_id": 100
    },
    {
      "id": 25,
      "name": "PREDY_STRATA",
      "label": "PREDY_STRATA",
      "descr": "PREDY with stratification",
      "business_service_id": 100
    },
    {
      "id": 30,
      "name": "SELEDIT",
      "label": "SELEDIT",
      "descr": "This function performs Selective Editing. On the basis of a set of observed data and the corresponding predictions for the true data, it selects the units required for interactive editing",
      "business_service_id": 100
    },
    {
      "id": 35,
      "name": "SELEDIT_STRATA",
      "label": "SELEDIT_STRATA",
      "descr": "SELEDIT with stratification",
      "business_service_id": 100
    },
    {
      "id": 70,
      "name": "CONTINGENCY_TABLE",
      "label": "CONTINGENCY_TABLE",
      "descr": "The first step of the probabilistic procedure consists of computing the comparison vector, given by the result of the function on the k matching variables, for all the pairs in the space. Indeed, starting from the vector distribution among the pairs (reported in the contingency table) the goal is the estimation of the probability distribution of the unknown random variable “link status”, which assigns each pair to the set M or to the set U. The comparison vector considered in RELAIS is a binary one, i.e. for each matching variable it reports the equality (corresponding to value 1) or the inequality (corresponding to value 0) between the units.",
      "business_service_id": 200
    },
    {
      "id": 71,
      "name": "FELLEGI_SUNTER",
      "label": "FELLEGI_SUNTER",
      "descr": "The Fellegi and Sunter method is a probabilistic approach to solve record linkage problem based on decision model.  According to the method, given two (or more) sources of data, all pairs coming from the Cartesian product of the two sources has to be classified in three independent and mutually exclusive subsets: the set of matches, the set of non-matches and the set of pairs requiring manual review. In order to classify the pairs, the comparisons on common attributes are used to estimate for each pair the probabilities to belong to both the set of matches and the set of non-matches. The pair classification criteria is based on the ratio between such conditional probabilities. The decision model aims to minimize both the misclassification errors and the probability of classifying a pair as belonging to the subset of pairs requiring manual review",
      "business_service_id": 200
    },
    {
      "id": 72,
      "name": "MATCHING_TABLE",
      "label": "MATCHING_TABLE",
      "descr": "Create result matching table",
      "business_service_id": 200
    }
  ],
  "parameter": [
    {
      "id": 1,
      "name": "MATCHING VARIABLES",
      "descr": "Matching variable algorithms ",
      "default_val": "3",
      "json_template": {
        "data": [],
        "schema": {
          "items": {
            "properties": {
              "MatchingVariable": {
                "maxLength": 32,
                "required": false,
                "title": "MatchingVariable",
                "type": "string"
              },
              "MatchingVariableA": {
                "maxLength": 331,
                "required": true,
                "title": "MatchingVariableA",
                "type": "string"
              },
              "MatchingVariableB": {
                "maxLength": 52,
                "required": true,
                "title": "MatchingVariableB",
                "type": "string"
              },
              "Method": {
                "enum": [
                  "Equality",
                  "Jaro",
                  "Dice",
                  "JaroWinkler",
                  "Levenshtein",
                  "3Grams",
                  "Soundex",
                  "NumericComparison",
                  "NumericEuclideanDistance"
                ],
                "required": true,
                "title": "Method"
              },
              "Threshold": {
                "title": "Threshold",
                "type": "number",
                "default": 1
              }
            },
            "type": "object"
          },
          "type": "array"
        },
        "options": {
          "type": "table",
          "showActionsColumn": false,
          "hideAddItemsBtn": true,
          "items": {
            "fields": {
              "Method": {
                "type": "select",
                "noneLabel": "",
                "removeDefaultNone": false
              },
              "MatchingVariableA": {
                "type": "select",
                "noneLabel": "",
                "dataSource": "matchedVariables"
              },
              "MatchingVariableB": {
                "type": "select",
                "noneLabel": "",
                "dataSource": "matchedVariables"
              }
            }
          },
          "form": {
            "buttons": {
              "addRow": "addRow",
              "removeRow": "removeRow"
            }
          },
          "view": {
            "templates": {
              "container-array-toolbar": "#addItemsBtn"
            }
          }
        }
      }
    },
    {
      "id": 2,
      "name": "THRESHOLD MATCHING",
      "descr": "THRESHOLD MATCHING",
      "default_val": "0.2",
      "json_template": {
        "data": [],
        "schema": {
          "name": "THRESHOLD MATCHING",
          "type": "number",
          "minimum": 0.001,
          "maximum": 2
        }
      }
    },
    {
      "id": 3,
      "name": "THRESHOLD UNMATCHING",
      "descr": "THRESHOLD UNMATCHING",
      "default_val": "1",
      "json_template": {
        "data": [],
        "schema": {
          "name": "THRESHOLD UNMATCHING",
          "type": "number",
          "minimum": 0.002,
          "maximum": 10
        }
      }
    },
    {
      "id": 101,
      "name": "MODEL",
      "descr": "DATA MODEeee",
      "default_val": "4",
      "json_template": {
        "data": [],
        "schema": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "layer": {
                "title": "Layer",
                "type": "string"
              },
              "N": {
                "title": "N",
                "type": "number"
              },
              "is.conv": {
                "title": "conv",
                "type": "boolean"
              },
              "lambda": {
                "title": "lambda",
                "type": "number"
              },
              "w": {
                "title": "w",
                "type": "number"
              },
              "B": {
                "title": "B",
                "type": "string"
              },
              "bic_norm": {
                "title": "bic_norm",
                "type": "string"
              },
              "bic_mix": {
                "title": "bic_mix",
                "type": "string"
              },
              "aic_norm": {
                "title": "aic_norm",
                "type": "string"
              },
              "aic_mix": {
                "title": "aic_mix",
                "type": "string"
              },
              "sigma": {
                "title": "sigma",
                "type": "number"
              }
            }
          }
        },
        "options": {
          "type": "table",
          "showActionsColumn": false
        }
      }
    },
    {
      "id": 102,
      "name": "INPUT_PARAMETERS",
      "descr": "INPUT PARAMETERS",
      "default_val": "",
      "json_template": {
        "data": [],
        "schema": {
          "properties": {
            "graph": {
              "required": true,
              "title": "Graph",
              "description": "Activates graphic output",
              "type": "number",
              "default": 0,
              "minimum": 0.01,
              "maximum": 10
            },
            "model": {
              "required": true,
              "title": "Model",
              "description": "Data Distribution: LN lognormal / N Normal",
              "default": "LN"
            },
            "tot": {
              "title": "Tot",
              "description": "Estimates of originals vector for the target variables"
            },
            "t.sel": {
              "title": "t.sel",
              "description": "Optional vector of threshold values for selective edinting on the target variables"
            },
            "t.outl": {
              "required": true,
              "title": "t.outl",
              "description": "Threshold value for posterior probabilities of identifying outliers",
              "type": "number",
              "default": 0.05,
              "minimum": 0.01,
              "maximum": 10
            },
            "eps": {
              "required": true,
              "title": "eps",
              "description": "Tolerance for the log-likelihood convergence",
              "type": "number",
              "default": 1e-7,
              "minimum": 1e-7,
              "maximum": 1
            },
            "lambda.fix": {
              "required": true,
              "title": "lambda.fix",
              "description": "TRUE if w is known",
              "type": "number",
              "default": 0,
              "maximum": 1
            },
            "w.fix": {
              "required": true,
              "title": "w.fix",
              "description": "TRUE if w is known",
              "type": "number",
              "default": 0,
              "minimum": 0.01,
              "maximum": 1
            }
          },
          "type": "object"
        }
      }
    },
    {
      "id": 103,
      "name": "OUTPUT_PARAMETERS",
      "descr": "OUTPUT PARAMETERS - INFO REPORT",
      "default_val": "",
      "json_template": {}
    }
  ],
  "step_instance": [
    {
      "id": 1,
      "method": "is2_mlest",
      "descr": "This function performs the maximum likelihood estimates of the parameters of a contamination model by ECM algorithm and it provides the expected values of the “true” data for all units that were used for the estimation",
      "label": "MLEST",
      "app_service_id": 100
    },
    {
      "id": 2,
      "method": "is2_ypred",
      "descr": "On the basis of a set of contamination model parameters, and a set of observed data, it calculates the expected values of the corresponding real data. Missing values for the variables response as well as are allowed, but not for covariates",
      "label": "PREDY",
      "app_service_id": 100
    },
    {
      "id": 3,
      "method": "is2_seledit",
      "descr": "This function performs Selective Editing. On the basis of a set of observed data and the corresponding predictions for the true data, it selects the units required for interactive editing",
      "label": "SELEDIT",
      "app_service_id": 100
    },
    {
      "id": 11,
      "method": "contingencyTable",
      "descr": "This function calculates the contingency Table",
      "label": "CONTINGENCY_TABLE",
      "app_service_id": 250
    },
    {
      "id": 12,
      "method": "fellegisunter",
      "descr": "This function implements the Fellegi Sunter algorithm",
      "label": "FELLEGI_SUNTER",
      "app_service_id": 200
    },
    {
      "id": 13,
      "method": "resultTables",
      "descr": "This function calculates the Matching Table",
      "label": "MATCHING_TABLE",
      "app_service_id": 250
    },
    {
      "id": 14,
      "method": "is2_validate_confront",
      "descr": "This function runs the confront algoritm implemented by Van Der Loo",
      "label": "Confront",
      "app_service_id": 300
    }
  ],
  "app_service": [
    {
      "id": 100,
      "name": "SeleMix",
      "descr": "Selemix is an R package to treat quantitative data, which aims to identify a set of units affected by errors which potentially influence the estimates of interest (selective editing)",
      "implementation_language": "R",
      "engine": "RSERVE",
      "source_path": "selemix/IS2_selemix.r",
      "source_code": "# Copyright 2019 ISTAT\n# \n#  Licensed under the EUPL, Version 1.1 or – as soon they will be approved by\n#  the European Commission - subsequent versions of the EUPL (the \"Licence\");\n#  You may not use this work except in compliance with the Licence. You may\n#  obtain a copy of the Licence at:\n# \n#  http://ec.europa.eu/idabc/eupl5\n# \n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the Licence is distributed on an \"AS IS\" basis, WITHOUT\n#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#  Licence for the specific language governing permissions and limitations under\n#  the Licence.\n# \n#  @author Francesco Amato <framato @ istat.it>\n#  @author Mauro Bruno <mbruno @ istat.it>\n#  @author Paolo Francescangeli  <pafrance @ istat.it>\n#  @author Renzo Iannacone <iannacone @ istat.it>\n#  @author Stefano Macone <macone @ istat.it>\n#   \n#  @version 1.0.0\n#\nrm(list=ls())\nlibrary(\"SeleMix\")\nlibrary(\"rjson\")\n\n#   Lista Ruoli\n#0\tSKIP\t\t\t\tN\tVARIABILE NON UTILIZZATA\n#1\tIDENTIFICATIVO\t\tI\tCHIAVE OSSERVAZIONE\n#2\tTARGET\t\t\t  Y\tVARIABILE DI OGGETTO DI ANALISI\n#3\tCOVARIATA\t\t  X\tVARIABILE INDIPENDENTE\n#4\tPREDIZIONE\t\t\tP\tVARIABILE DI PREDIZIONE\n#5\tOUTLIER\t\t\t  O\tFLAG OUTLIER\n#6\tPESO\t\t\t\tW\tPESO CAMPIONARIO\n#7\tERRORE\t\t\t  E\tERRORE INFLUENTE\n#8\tRANKING\t\t\t  R\tINFLUENCE RANKING\n#9\tOUTPUT\t\t\t  T\tVARIABILE DI OUTPUT\n#10\tSTRATO\t\t\t  S\tPARTIZIONAMENTO DEL DATASET\n#11\tPARAMETRI\t\t  Z\tPARAMETRI DI ESERCIZIO\n#12\tMODELLO\t\t\t  M\tMODELLO DATI\n#13\tSCORE\t\t\t  F\tINFLUENCE SCORE\n#14\tREPORT\t\t\t  G\tPARAMETRO DI OUTPUT / REPORT\n\n\n# Lista oggetti Bridge Java - R\n# SELEMIX_RESULTSET = \"sel_out\";\n# SELEMIX_WORKSET =   \"workset\";\n# SELEMIX_RUOLI_VAR = \"role_var\";\n# SELEMIX_RUOLI_VAR_OUTPUT = \"role_var_out\";\n# SELEMIX_RUOLI_INPUT = \"role_in\";\n# SELEMIX_RUOLI_OUTPUT = \"ruol_out\";\n# SELEMIX_PARAMETRI = \"array_par\";\n# SELEMIX_MODELLO = \"array_mod\";\n\n\n### ESEMPI DI MODELLI MONO E MULTIVARIATI\n\n#MODELLO MULTIVARIATO 2 VAR TARGET\n#B <- c(1.78840493, -0.065592887, 0.74442347, -0.009121287, -0.04293598,  1.048079464)\n#sigma <- c(0.17068817, 0.03489681, 0.03489681, 0.49760310)\n#lambda <- 19.96269\n#w <- 0.2122271\n\n#MODELLO MULTIVARIATO CON 2 VAR TARGET SENZA COVAR\n#B <- c(5.973958, 5.112587)\n#sigma <- c(0.3867086, -0.2527695, -0.2527695,  1.2920025)\n#lambda <- 11.205\n#w <- 0.332\n\n#MODELLO MONOVARIATO\t\n#B <- c(-0.152, 1.215)\n#sigma <- c(1.25)\n#lambda <- 15.5\n#w <- 0.0479\n\n#MODELLO MONOVARIATO SENZA COVAR\t\n#B <- c(-0.152)\n#sigma <- c(1.25)\n#lambda <- 15.5\n#w <- 0.0479\n\n# Impostazione manuale dataset \n#workset <- read.csv2('C:/Users/pafrance/photon-workspace/Rscripts/input.strata.csv', dec=\".\", sep=\";\")\n\n# imposta variabili di ruolo (specifica manualmente chi è target, covar ecc ecc)\n#Y <- \"Y1\"\n#X <- \"X1\"\n#S <- \"S1\"\n\n#prova passaggio parametri in json\nml_est_json = function(original_request) {\n    request = fromJSON(original_request)\n    \n    final_result <- is2_mlest(request$workset, request$roles, request$wsparams)\n    \n    JSON_result = toJSON(final_result)\n    return(JSON_result)\n}\n\n\n\n\n#stima completa\nis2_mlest_ori <- function( workset, roles, wsparams=NULL,...) {\n \t\n\tstdout <- vector('character')\n\tcon <- textConnection('stdout', 'wr', local = TRUE)\n\t\n    x <- workset[roles$X]\n\ty <- workset[roles$Y]\n\t\n\t#Default params\n\tmodel=\"LN\"\n\tt.outl=0.5\n\tlambda=3\n\tw=0.05\n\tlambda.fix=FALSE\n\tw.fix=FALSE\n\teps=1e-7\n\tmax.iter=500\t\n\t\n\t#Parameter check\n\tprint(wsparams)\n\tif(!is.null(wsparams)){\n\t\n\tprint(\"asdfffffffffffffffffffffffffffffffffffffffffffff\")\n\tprint(wsparams$model)\n\t\tif(exists(\"wsparams$model\")) model=wsparams$model\n\t\tif(exists(\"wsparams$t.outl\")) t.outl=wsparams$t.outl\n\t\tif(exists(\"wsparams$lambda\")) lambda=wsparams$lambda\n\t\tif(exists(\"wsparams$w\")) w=wsparams$w\n\t\tif(exists(\"wsparams$lambda.fix\")) lambda.fix=wsparams$lambda.fix\n\t\tif(exists(\"wsparams$w.fix\")) w.fix=wsparams$w.fix\n\t\tif(exists(\"wsparams$eps\")) eps=wsparams$eps\n\t\tif(exists(\"wsparams$max.iter\")) max.iter=wsparams$max.iter\n \t}\n\t\n\t#Execute algorithm (mettere un try catch)\n\t\n\test <- ml.est(y=y, x=x, model = model, lambda= as.numeric(lambda),  w= as.numeric(w), lambda.fix=lambda.fix, w.fix=w.fix, eps=as.numeric(eps), max.iter=as.numeric(max.iter), t.outl= as.numeric(t.outl), graph=FALSE)\n\tif(length(workset)>1) ypred <- matrix(est$ypred,nrow=nrow(workset),ncol=length(roles$Y))\n\telse ypred <- as.matrix(est$ypred)\n\t\n\t#reimpostazione nomi delle variabili\n\toutp <- data.frame(tau=est$tau, outlier=est$outlier, pattern=est$pattern)\n\tpredname = c()\n\tout1 = c()\n\tfor(i in 1:ncol(ypred)) {\n\t\tpred = ypred[,i]\n\t\tpredname = c(predname, paste(\"YPRED\",i,sep=\"_\"))\n\t\tout1 <- cbind(out1,pred)\n\t}\n\tout1=data.frame(out1)\n\tcolnames(out1) <- predname\n\t#output parameters\n\treport <- list(n.outlier = sum(est$outlier), missing = sum(as.numeric(est$pattern)),  is.conv = est$is.conv, sing = est$sing, bic.aic = est$bic.aic)\n\tmod <- list(B=est$B, sigma=est$sigma, lambda=est$lambda, w=est$w )\n\tparam_mod <- list( Model = toJSON(mod))\n    #param_mod <- mod\t\n\t\n\tparam_report <- list( Report = toJSON(report))\n\t#param_report <-  report\n\t#setting output roles \n\t \n\t#roles <- list (P= c(roles$X,roles$Y, predname,names(outp)), O=\"outlier\", M=names(mod), G=names(report))\n\troles <- list (P= c(roles$X,roles$Y, predname,names(outp)), O=\"outlier\", M=\"Model\",G=\"Report\")\n\tr_out<-cbind(x,y,outp,out1)\n\trolesgroup <- list (P= c(\"P\", \"O\"),  M=\"M\",G=\"G\")\n\tprint(rolesgroup)\n\t#result <-list( workset_out=r_out, roles_out=roles,rolesgroup_out=rolesgroup, params_out=mod, report=report, log=stdout)\n\tresult <-list( workset_out=r_out, roles_out=roles,rolesgroup_out=rolesgroup, params_out=param_mod, report_out=param_report, log=stdout)\n\t\n\tsink()\n\tclose(con)\n\treturn(result)\n}\n\n\n\n\n#stima completa con layer\nis2_mlest <- function( workset, roles, wsparams=NULL,...) {\n \t\n\tstdout <- vector('character')\n\tcon <- textConnection('stdout', 'wr', local = TRUE)\n\t\t\n\t#Default params\n\tmodel=\"LN\"\n\tt.outl=0.5\n\tlambda=3\n\tw=0.05\n\tlambda.fix=FALSE\n\tw.fix=FALSE\n\teps=1e-7\n\tmax.iter=500\t\n\t\n\t#Parameter check\n\tif(!is.null(wsparams)){\n\n\t\tif(exists(\"wsparams$model\")) model=wsparams$model\n\t\tif(exists(\"wsparams$t.outl\")) t.outl=wsparams$t.outl\n\t\tif(exists(\"wsparams$lambda\")) lambda=wsparams$lambda\n\t\tif(exists(\"wsparams$w\")) w=wsparams$w\n\t\tif(exists(\"wsparams$lambda.fix\")) lambda.fix=wsparams$lambda.fix\n\t\tif(exists(\"wsparams$w.fix\")) w.fix=wsparams$w.fix\n\t\tif(exists(\"wsparams$eps\")) eps=wsparams$eps\n\t\tif(exists(\"wsparams$max.iter\")) max.iter=wsparams$max.iter\n \t}\n\t\n\t# get DATASET \n    \n\ts <- workset[[roles$S]]\n\t\n \tlayers <- sort(unique(s))\n\n    r_out <-c()\n    print(layers)\n\t mod <-c()\n\tparam_mod <-c()\n\tfor(layer in layers){\n\t\trm(workset_layer)\n\t\trm(x)\n\t\trm(y)\n\t\trm(s1)\n\t\trm(outp)\n\t\trm(out1)\n\t\t \n\t    workset_layer <- workset[workset[roles$S]==layer, , drop = TRUE ]\n\t  \n\t    x <- workset_layer[roles$X]\n\t\ty <- workset_layer[roles$Y]\n\t\ts1 <- workset_layer[roles$S]\n\t\t\n\t\t#Execute algorithm (mettere un try catch)\n\t\t\n\t\test <-  try(ml.est(y=y, x=x, model = model, lambda= as.numeric(lambda),  w= as.numeric(w), lambda.fix=lambda.fix, w.fix=w.fix, eps=as.numeric(eps), max.iter=as.numeric(max.iter), t.outl= as.numeric(t.outl), graph=FALSE))\n\t\tpredname = c()\n\t\tout1 = c()\n\t\tif((\"est\"%in%ls() & class(est)[1]!=\"try-error\")){\n\t\t    if(length(workset_layer)>1) ypred <- matrix(est$ypred,nrow=nrow(workset_layer),ncol=length(roles$Y))\n\t\t\telse ypred <- as.matrix(est$ypred)\n\t\t\t\n\t\t\t#reimpostazione nomi delle variabili\n\t\t\toutp <- data.frame(tau=est$tau, outlier=est$outlier, pattern=est$pattern)\n\t\t\t\n\t\t\tfor(i in 1:ncol(ypred)) {\n\t\t\t\tpred = ypred[,i]\n\t\t\t\tpredname = c(predname, paste(\"YPRED\",i,sep=\"_\"))\n\t\t\t\tout1 <- cbind(out1,pred)\n\t\t\t}\n\t\t\tout1=data.frame(out1)\n\t\t\tcolnames(out1) <- predname\n\t\t}\n\t\telse{\n\t\t\t outp <- data.frame(tau=NA, outlier=NA, pattern=NA)\n\t\t}\n\t\t\t#output \n\t\t\tr_out<- rbind(r_out, cbind(x,y,s1,outp,out1))\n\t\t\t\n\t\t\t\t \n\t\t\t#output parameters\n\t\t\tmod <- rbind(mod, toJSON(list(layer=layer,B=est$B, sigma=est$sigma, lambda=est$lambda, w=est$w )))\n\t\t\t\n\t\t\t#Report output \n\t\t\treport <- list(n.outlier = sum(est$outlier), missing = sum(as.numeric(est$pattern)),  is.conv = est$is.conv, sing = est$sing, bic.aic = est$bic.aic)\n\t\t\tparam_report <- list( Report = toJSON(report))\n\t\t\n\t} \n  \t\tparam_mod <- list( Model = toJSON(mod))\n\t\tprint(\tparam_mod)\n\t#setting output roles \n\troles <- list (P= c(roles$X,roles$Y,roles$S, predname,names(outp)), O=\"outlier\", M=\"Model\",G=\"Report\")\n\t#setting output rolesgroup \n\trolesgroup <- list (P= c(\"P\", \"O\"),  M=\"M\",G=\"G\")\n\t\t\n\t#result \n\tresult <-list( workset_out=r_out, roles_out=roles,rolesgroup_out=rolesgroup, params_out=param_mod, report_out=param_report, log=stdout)\n\t\n\tsink()\n\tclose(con)\n\treturn(result)\n}\n\n#esecuzione strato\nstrata.mlest <- function(workset, y, x=NULL, s, ...) {\n  #sistemazione dell'input\n  strata <- as.factor(workset[,S])\n  workset[,Y] <- as.numeric(workset[,Y])\n  workset[,X] <- as.numeric(workset[,X])\n  #init data\n  df <- data.frame()\n  mod <- data.frame()\n  report <- data.frame()\n  #esegue MLEST sullo strato\n  for (i in levels(strata)) {\n    w <- workset[workset[,S]==i, , drop = TRUE ]\n  \test1<- mlest(w, Y, X)\n    df <- rbind(df,  cbind(w, est1$out)) #ricreazione del dataset \n  }\n  #costruisce la lista di ritorno\n  result <-list( out= df, roles= est1$roles, mod = est1$mod, report = est1$report)\n  return(result)\n}\n\n#Predizione da modello\nis2_ypred <- function(workset, y, x=NULL, ... ) {\n\tif(!exists(\"model\"))  model=\"LN\"\n\tif(!exists(\"t.outl\"))  t.outl=0.5\n\t\n\t#environment check\n\tif(missing(y)) stop('iSS Error: Missing TARGET Variable(s)')\n\ty <- matrix(as.numeric(workset[,Y]),ncol=length(Y),nrow=nrow(workset))\n\tif(!missing(x)) x <- matrix(as.numeric(workset[,X]),ncol=length(X),nrow=nrow(workset))\n\t\n\tif((exists('B')& exists('sigma')& exists('lambda')& exists('w'))) {\n\t\tsizex = ifelse(exists(\"X\"), length(X), 0)\n\t\tbeta <- matrix(as.numeric(B), nrow=1+sizex, ncol=length(Y), byrow=TRUE)\n\t\ts <- matrix(as.numeric(sigma), nrow=length(Y),  ncol=length(Y), byrow=TRUE)\n\t\tl <- as.numeric(lambda)\n\t\tv <- as.numeric(w)\n\t}\n\telse {\n\t\tstop('iSS Error: Missing model')\n\t}\n\test <- pred.y(y=y, x=x, sigma = s, B = beta, model = model, lambda= l,  w= v, t.outl= as.numeric(t.outl))\n\tout <- data.frame(est) \n\n\t#setting output roles\n\treport <- list(n.outlier = sum(est$outlier), missing = sum(as.numeric(est$pattern)) )\n\troles <- list (P=colnames(out)[1:length(Y)], O= \"outlier\", G=names(report))\n\tresult <- list( out=out, roles= roles, report = report)\n\treturn(result)\n}\n\n#esecuzione strato\nstrata.ypred <- function(workset, y, x=NULL, s, ...) {\n  #sistemazione dell'input\n  strata <- as.factor(workset[,S])\n  #workset[,S] <- as.factor(workset[,S])\n  workset[,Y] <- as.numeric(workset[,Y])\n  workset[,X] <- as.numeric(workset[,X])\n  \n  #esempio di selezione per strato\n  df <- data.frame()\n  report <- data.frame()\n  #esegue YPRED sullo strato\n  for (i in levels(strata)) {\n  \tw <- workset[workset[,S]==i, , drop = TRUE ]\n  \test1<- ypred(w, Y, X) #ypred(w, y=y, x=x, sigma = s, B = beta, model = model, lambda= l,  w= v, t.outl= as.numeric(t.outl))\n    df <- rbind(df,  cbind(w, est1$out)) #ricreazione del dataset \n  }\n  result <-list( out=df, roles= est1$roles, report = est1$report)\n  return(result)\n}\n\n#editing selettivo\nis2_seledit <- function(workset, y, p, ...) {\n    #controllo environment\n\tif(missing(y)|missing(p)) {\n\t\tstop('iSS Error: Missing TARGET or PREDICTION Variable(s)')\n\t}\n\tif(length(y)!=length(p)) {\n\t\tstop('iSS Error: Input dimension mismatch')\n\t}\n\t\n\tdata <- matrix(as.numeric(workset[,Y]),ncol=length(Y),nrow=nrow(workset))\n\typred <- matrix(as.numeric(workset[,P]),ncol=length(P),nrow=nrow(workset))\n\tif(!exists(\"wgt\"))  wgt=rep(1,nrow(workset))\n\tif(!exists(\"tot\"))  tot=colSums(ypred * wgt) \n\tif(!exists(\"t.sel\"))  t.sel=0.01\n\t\n\twhat <- sel.edit (y=data, ypred=ypred, wgt, tot, t.sel= as.numeric(t.sel))\n\tout= data.frame(what)\n\t\n\t#setting output roles\n\treport <- list(n.error = sum(out$sel))\n\troles <- list (E=\"sel\", R= \"rank\", F=\"global.score\", G=names(report))\n\tresult <-list( out= out, roles= roles, report = report)\n\treturn(result)\n}\n\n#esecuzione strato\nstrata.seledit <- function(workset, y, p, s, ...) {\n  #sistemazione dell'input\n  strata <- as.factor(workset[,S])\n  workset[,Y] <- as.numeric(workset[,Y])\n  workset[,P] <- as.numeric(workset[,P])\n\n  #esempio di selezione per strato\n  df <- data.frame()\n  #report <- data.frame()\n  #esegue SELEDIT sullo strato\n  for (i in levels(strata)) {\n  \tw <- workset[workset[,S]==i, , drop = TRUE ]\n  \test1<- seledit(w, Y, P) \n    df <- rbind(df,  cbind(w, est1$out)) #ricreazione del dataset \n  }\n  result <-list( out=df, roles= est1$roles, report = est1$report )\n  return(result)\n}\n\n#esecuzione di prova in ambiente R\n#est2 <- strata.mlest(workset, Y, X, S)",
      "author": "Istat",
      "licence": "EUPL1.1",
      "contact": "Maria Teresa Buglielli (bugliell@istat.it)",
      "business_service_id": 100
    },
    {
      "id": 200,
      "name": "Relais",
      "descr": "R package implementing record linkage methods",
      "implementation_language": "R",
      "engine": "RENJIN",
      "source_path": "relais/relais.R",
      "source_code": "library(varhandle)\n#eliminazione di ogni precedente elaborazione (pulizia memoria)\nrm(list=ls())\n#Programma valido per una variabile latente dicotomica e k variabili manifeste dicotomiche\n#vincolo sul numero di variabili: numero variabili >= 3\n#lettura del file contenente i parametri di input:\n#nvar= numero variabili manifeste\n#contingencyTableName= nome della tabella che contiente la matrice di contingenza\n#muTableName=nome della tabella che contenente i parametri stimati, nel caso di stima **affidabile** del modello\n#percorso_fail=path del file contenete le condizionate alla variabile latente, nel caso di stima ** non affidabile** del modello\n#eps,iter parametri del modello EM con valori di default\n\n\n#percorso_fail=\"FSFail.Rout\"\n#percorso_allert=\"FSAllert.Rout\"\n\nfellegisunter <- function(workset, roles, wsparams=NULL, ...) {\n stdout <- vector('character')\n con <- textConnection('stdout', 'wr', local = TRUE)\n sink(con)\n\n ct <- roles$CT\n print(ct)\n nvar=length(ct)-1\n #yy <-  as.data.frame(matrix(as.numeric(workset[,ct]),ncol=length(ct),nrow=nrow(workset)))\n yy <- workset\n #print(workset)\n colnames(yy)<- ct\n #print(yy)\t \n\tmuTableName=\"muTable\"\n\tvarmuTableName=\"varmuTable\"\n\tr_out <- vector()\n\t\n\teps=0.0000001\n\titer=1500\n\tinterazioni=vector(\"list\", nvar)\n\tfor (i in (1:nvar))\n\tinterazioni[[i]]=c(nvar+1,i)\n\t\n\t#inizializzazioni delle variabili per le iterazioni del metdodo EM\n\ti=1\n\tstop=0\n\tynew=0\n\t\n\t#Crea alcune variabili stringa utili per la parametrizzazione del programma\n\tvariabili<-paste('Freq',paste(paste('V',nvar:1,sep=''),collapse='+'),sep='~')\n\t#print(variabili)\n\t\n\tnomimatvar = names(yy)[1:nvar]\n\tprint( names(yy))\n\t \n\t#aggiorno i valori di frequency a o\n\tif(nrow(yy[yy$FREQUENCY==0,])>0)\n\t  yy[yy$FREQUENCY==0,]$FREQUENCY<-0.0001\n\t \n\t#legge i dati come data frame\n\tnames(yy) [nvar+1] = 'Freq'\n\tfor(i in 1:nvar) names(yy)[i] = paste('V',i,sep='')\n\t#crea la matrice yy1 con la variabile latente\n\tyy1=rbind(yy,yy)\n\tx<-as.integer(gl(2,2^nvar,2^(nvar+1)))-1\n\tyy1=cbind(x,yy1)\n\t\n\t#assegna i valori iniziali alla matrice yy1\n\tyy2=as.matrix(log(yy1[,2:(nvar+1)]*ifelse(x>0.9,0.8,0.2)+((1-yy1[,2:(nvar+1)])*ifelse(x>0.9,0.2,0.8))))%*%matrix(rep(1,nvar),nvar)\n\tyy1[,nvar+2]=((yy1[,1]*0.1+(1-yy1[,1])*0.9)*exp(yy2))*sum(yy1[,nvar+2])/2\n\t\n\t#passo M - modello loglineare sulla tabella completa\n\twhile (stop == 0) {\n\t   y<-xtabs(as.formula(paste(variabili,'x',sep='+')), data=yy1);\n\t   stepm=loglin(y,interazioni,fit=T,print=F);\n\t   diff=abs(max(ynew-stepm$fit));\n\t   ynew=stepm$fit;\n\t   ycond=xtabs(as.formula(variabili), data=yy)/xtabs(as.formula(variabili), data=ynew); \n\t   Freq=rbind(as.matrix(ynew[1:(2^nvar)]*ycond),as.matrix(ynew[((2^nvar)+1):(2^(nvar+1))]*ycond));\n\t   yy1=cbind(yy1[,1:(nvar+1)],Freq);\n\t   i=i+1;\n\t   stop=ifelse(diff>eps,0,1)+ifelse(i<=iter,0,1)\n\t}\n\t\n\t#probabilità condizionate alla variabile latente\n\t#xtabs(Freq~x, data=yy1)/ sum(yy1[,(nvar+2)])\n\tl=list()\n\tvett=vector(\"numeric\")\n\tmvar <- vector(\"numeric\")\n\tuvar <- vector(\"numeric\")\n\t\n\tfor(i in 1:nvar) {\n\t\ttabella=paste('Freq~V',i,sep='');\n\t\ttabella=paste(tabella,'+x',sep='');\n\t\ttab=xtabs(as.formula(tabella), data=yy1)/ rbind(xtabs(Freq~x, data=yy1),xtabs(Freq~x, data=yy1));\n\t\tl[[i]]=tab\n\t\tvett=c(vett,tab)\n\t\tmvar <- c(mvar,tab[2,2],tab[1,2])\n\t\tuvar <- c(uvar,tab[2,1],tab[1,1])\n\t}\n\t\n\t## salvataggio della tabella delle marginali spostata sotto\n\t\n\t#Verifica condizione di stima inaffidabile dei parametri\n\t\n\t#1 check su stime v1-vn e var latente x (vedi blocco laura)\n\tcheck = 0;\n\tprint(nvar)\n\tfor(i in 1:nvar){\n\tif((l[[i]][1,1]>=l[[i]][2,1] && l[[i]][1,2]>=l[[i]][2,2]) || (l[[i]][1,1]<l[[i]][2,1] && l[[i]][1,2]<l[[i]][2,2]))\n\t check = 1;\n\t}\n\tif(check==1){\n\t\t#Messaggio di warning non blocante\n\t\t# msg = \"WARNING: one or more variables give inconsistent estimates.\"\n\t\t#Messaggio di errore blocante\n\t\t msg = \"ERROR: one or more variables give inconsistent estimates. Please, check the variables in the model or try to reduce the search space.\";\n\t\t #msg2 = paste(\"See table \",varmuTableName,\" for more details.\"); \n\t\t print(msg);\n\t\t #print(msg2);\n\t\t print(l);\n\t\t #write.table(l, file = percorso_fail,row.names=FALSE,sep=\":\",quote=F)\n\t\t #default value to p for the marginal prob table\n\t\t p <- 0\n\t}else{\n\t\t#2 check su stime sulla frontiera\n\t\t#Messaggio di warning non blocante\n\t\t\n\t\tvett_cond=vett[vett>0.99999]\n\t\tif (length(vett_cond)>=1)\n\t\t{\n\t\t# messaggio di warning non bloccante, si salvano i parametri nella tabella, e si va avanti con la mutable\n\t\t\tmsg3 = paste(\"WARNING: one or more nearly boundary parameters. See \",percorso_allert,\" for more details.\");\n\t\t\tprint(msg3);\n\t\t\tprint(l);\n\t\t   #  write.table(l, file = percorso_allert,row.names=FALSE,sep=\":\",quote=F)\n\t\t}\n\t\t#produce la matrice in output\n\t\t#per la stima di m e u utilizziamo le frequenze attese dal modello\n\t\t# mentre nella tabella mu appaiono le frequenza f_m f_u riproporzionate sui valori osservati\n\t\t# f_u_att e f_u_obs differiscono tanto più quanto più é scarso il fit del modello\n\t\tf_u_att=as.matrix(ynew[1:2^nvar])\n\t\tf_m_att=as.matrix(ynew[((2^nvar)+1):(2^(nvar+1))])\n\t\tu=f_u_att/sum(f_u_att)\n\t\tm=f_m_att/sum(f_m_att)\n\t\tp=sum(f_m_att)/sum(yy1[,(nvar+2)])\n\t\tr=m/u\n\t\tp_post=m*p/(m*p+u*(1-p))\n\t\tu=round(u, digits=5)\n\t\tm=round(m, digits=5)\n\t\tr=round(r, digits=5)\n\t\tp_post=round(p_post, digits=5)\n\t\tf_u_obs=as.matrix(ynew[1:2^nvar]*ycond)\n\t\tf_m_obs=as.matrix(ynew[((2^nvar)+1):(2^(nvar+1))]*ycond)\n\t\tf_m_obs=round(f_m_obs, digits=5)\n\t\tf_u_obs=round(f_u_obs, digits=5)\n\t\tr_out=cbind(yy[,1:nvar],f_m_obs,f_u_obs,m,u,r,p_post)\n\t\t\n\t\tr_out =r_out [order(r_out$r , decreasing = FALSE ),]\n\t\t\n\t\tprec_oss <- rep(0,2^nvar)\n\t\trecl_oss <- rep(0,2^nvar)\n\t\tfor (j in 1:2^nvar) {\n\t\t   prec_oss[j] <- sum(r_out$f_m_obs[j:2^nvar])/(sum(r_out$f_m_obs[j:2^nvar])+sum(r_out$f_u_obs[j:2^nvar]))\n\t\t   recl_oss[j] <- sum(r_out$f_m_obs[j:2^nvar])/sum(r_out$f_m_obs)\n\t\t}\n\t\t\n\t\tprec_oss=round(prec_oss, digits=5)\n\t\trecl_oss=round(recl_oss, digits=5)\n\t\t\n\t\tr_out <- cbind(r_out,prec_oss,recl_oss)\n\t\tnames(r_out)[-(1:nvar)] <- c(\"F_M\",\"F_U\",\"M\",\"U\",\"R\",\"P_POST\",\"PRECISION\",\"RECALL\")\n\t\t\n\t\tmsgP = paste(\"The match frequency estimated from EM algorithm is p = \",round(p, digits=6));\n\t\tprint(msgP);\n\t\t\n\t\t# inserisco i nomi delle matvar per creare la tabella\n\t\tnames(r_out)[1:nvar]=nomimatvar\n\t\t\n\t}\n\t\n\t# creo la tabella delle marginali\n\tvar_est <- data.frame(rep(nomimatvar, rep(2,length(nomimatvar))),rep(c(\"1\",\"0\"),length(nomimatvar)),mvar,uvar,rep(p,2*length(nomimatvar)),stringsAsFactors=FALSE)\n\tnames(var_est)=c(\"variable\",\"comparison\",\"m\",\"u\",\"p\")\n\t\n roles <- list (FS=names(r_out))\n rolesgroup <- list (FS= c(\"FS\"))\n\n result <-list( workset_out=r_out, roles_out= roles,rolesgroup_out= rolesgroup, var_est = var_est, log = stdout)\n \n sink()\n close(con)\n return(result)\n \n}",
      "author": "Istat",
      "licence": "EUPL1.1",
      "contact": "Luca Valentino (luvalent@istat.it)",
      "business_service_id": 200
    },
    {
      "id": 250,
      "name": "Relais Java",
      "descr": "Java package implementing record linkage methods",
      "implementation_language": "JAVA",
      "engine": "JAVA",
      "source_path": "it.istat.is2.catalogue.relais.service.RelaisService",
      "source_code": "",
      "author": "Istat",
      "licence": "EUPL1.1",
      "contact": "Luca Valentino (luvalent@istat.it)",
      "business_service_id": 200
    },
    {
      "id": 300,
      "name": "Validate",
      "descr": "R package implementing a set of data validation functions",
      "implementation_language": "R",
      "engine": "RSERVE",
      "source_path": "validate/validate.r",
      "source_code": "",
      "author": "Mark van der Loo",
      "licence": "GPL-3",
      "contact": "Mark van der Loo (mark.vanderloo@gmail.com)",
      "business_service_id": 300
    }
  ],
  "app_role": [
    {
      "id": 1,
      "code": "X",
      "name": "MATCHING VARIABLES",
      "descr": "MATCHING VARAIBLES",
      "order_code": 1,
      "cls_data_type_id": 2,
      "parameter_id": 1
    },
    {
      "id": 2,
      "code": "X1",
      "name": "MATCHING A",
      "descr": "MATCHING VARIABLE IN DATASET A",
      "order_code": 2,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 3,
      "code": "X2",
      "name": "MATCHING B",
      "descr": "MATCHING VARIABLE IN DATASET B",
      "order_code": 3,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 4,
      "code": "CT",
      "name": "CONTENGENCY TABLE",
      "descr": "CONTENGENCY TABLE",
      "order_code": 4,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 5,
      "code": "FS",
      "name": "FELLEGI-SUNTER",
      "descr": "FELLEGI-SUNTER",
      "order_code": 2,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 6,
      "code": "B",
      "name": "BLOCKING",
      "descr": "SLICING DEL DATASET",
      "order_code": 7,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 7,
      "code": "MT",
      "name": "MATCHING TABLE",
      "descr": "MATCHING TABLE",
      "order_code": 10,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 8,
      "code": "TH",
      "name": "THRESHOLD MATCHING",
      "descr": "THRESHOLD MATCHING",
      "order_code": 9,
      "cls_data_type_id": 2,
      "parameter_id": 2
    },
    {
      "id": 9,
      "code": "TU",
      "name": "THRESHOLD UNMATCHING",
      "descr": "THRESHOLD UNMATCHING",
      "order_code": 10,
      "cls_data_type_id": 2,
      "parameter_id": 3
    },
    {
      "id": 10,
      "code": "PM",
      "name": "POSSIBLE MATCHING TABLE",
      "descr": "POSSIBLE MATCHING TABLE",
      "order_code": 9,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 11,
      "code": "M",
      "name": "RANKING",
      "descr": "INFLUENCE RANKING",
      "order_code": 5,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 12,
      "code": "S",
      "name": "STRATA",
      "descr": "PARTIZIONAMENTO DEL DATASET",
      "order_code": 6,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 13,
      "code": "RA",
      "name": "RESIDUAL A",
      "descr": "RESIDUAL DATASET  A",
      "order_code": 6,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 14,
      "code": "RB",
      "name": "RESIDUAL B",
      "descr": "RESIDUAL DATASET  B",
      "order_code": 5,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 15,
      "code": "MD",
      "name": "DATA",
      "descr": "DATA",
      "order_code": 1,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 16,
      "code": "RS",
      "name": "RULESET",
      "descr": "RULESET",
      "order_code": 2,
      "cls_data_type_id": 4,
      "parameter_id": 0
    },
    {
      "id": 100,
      "code": "N",
      "name": "SKIP",
      "descr": "VARIABILE NON UTILIZZATA",
      "order_code": 100,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 101,
      "code": "I",
      "name": "IDENTIFICATIVO",
      "descr": "CHIAVE OSSERVAZIONE",
      "order_code": 1,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 102,
      "code": "Y",
      "name": "TARGET",
      "descr": "VARIABILE DI OGGETTO DI ANALISI",
      "order_code": 3,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 103,
      "code": "X",
      "name": "COVARIATA",
      "descr": "VARIABILE INDIPENDENTE",
      "order_code": 4,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 104,
      "code": "P",
      "name": "PREDIZIONE",
      "descr": "VARIABILE DI PREDIZIONE",
      "order_code": 5,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 105,
      "code": "O",
      "name": "OUTLIER",
      "descr": "FLAG OUTLIER",
      "order_code": 6,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 106,
      "code": "W",
      "name": "PESO",
      "descr": "PESO CAMPIONARIO",
      "order_code": 7,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 107,
      "code": "E",
      "name": "ERRORI INFLUENTI",
      "descr": "ERRORE INFLUENTE",
      "order_code": 10,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 108,
      "code": "R",
      "name": "RANKING",
      "descr": "INFLUENCE RANKING",
      "order_code": 11,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 109,
      "code": "T",
      "name": "OUTPUT",
      "descr": "VARIABILE DI OUTPUT",
      "order_code": 20,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 110,
      "code": "S",
      "name": "STRATO",
      "descr": "PARTIZIONAMENTO DEL DATASET",
      "order_code": 2,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 111,
      "code": "Z",
      "name": "PARAMETRI",
      "descr": "PARAMETRI DI INPUT",
      "order_code": 997,
      "cls_data_type_id": 2,
      "parameter_id": 102
    },
    {
      "id": 112,
      "code": "M",
      "name": "MODELLO",
      "descr": "MODELLO DATI",
      "order_code": 998,
      "cls_data_type_id": 2,
      "parameter_id": 101
    },
    {
      "id": 113,
      "code": "F",
      "name": "SCORE",
      "descr": "INFLUENCE SCORE",
      "order_code": 12,
      "cls_data_type_id": 1,
      "parameter_id": 0
    },
    {
      "id": 114,
      "code": "G",
      "name": "INFO",
      "descr": "PARAMETRI OUT - INFO RIEPILOGO",
      "order_code": 999,
      "cls_data_type_id": 2,
      "parameter_id": 103
    },
    {
      "id": 115,
      "code": "V",
      "name": "CONVERGENZA",
      "descr": "VARIABILE DI CONVERGENZA",
      "order_code": 6,
      "cls_data_type_id": 1,
      "parameter_id": 0
    }
  ]
}